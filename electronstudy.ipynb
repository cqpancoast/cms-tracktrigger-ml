{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electron Study\n",
    "\n",
    "The aim of this study is to show that the introduction of new track properties, namely `chi2rphi` and `chi2rz`, improve the performance of machine learning algorithms dedicated to telling whether electron-labeled track-trigger tracks are real or fake.\n",
    "\n",
    "The Monte-Carlo samples used are a QCD sample, a Z boson to muon-muon sample, and a Z boson to electron-electron sample. Each sample is run for the D49 detector geometry and contains 1000 events, each of which has a pileup of about 200.\n",
    "\n",
    "Much of the code here is run using the `ntupledicts` package, which can be found [here](https://github.com/cqpancoast/ntupledicts), along with a simple tutorial that covers all code used here.\n",
    "The requirements for running this notebook are the same as the ones listed for running `ntupledicts` in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uproot import open as uproot_open\n",
    "from matplotlib.pyplot import cla, sca, gca, savefig\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from ntupledicts import operations as ndops\n",
    "from ntupledicts.operations import select as sel\n",
    "from ntupledicts import plot as ndplot\n",
    "from ntupledicts.ml import data as ndmldata\n",
    "from ntupledicts.ml import predict as ndmlpred\n",
    "from ntupledicts.ml import models as ndmlmodels\n",
    "from ntupledicts.ml import plot as ndmlplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "Grab tracks from stored ntuples, perform cuts, process into datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trk\n",
      "['pt', 'eta', 'z0', 'nstub', 'genuine', 'matchtp_pdgid', 'chi2', 'bendchi2', 'chi2rphi', 'chi2rz']\n",
      "trk\n",
      "['pt', 'eta', 'z0', 'nstub', 'genuine', 'matchtp_pdgid', 'chi2', 'bendchi2', 'chi2rphi', 'chi2rz']\n",
      "trk\n",
      "['pt', 'eta', 'z0', 'nstub', 'genuine', 'matchtp_pdgid', 'chi2', 'bendchi2', 'chi2rphi', 'chi2rz']\n"
     ]
    }
   ],
   "source": [
    "# List the ntuples we want data from\n",
    "input_files = [\"eventsets/ZMM_PU200_D49.root\",\n",
    "    \"eventsets/ZEE_PU200_D49.root\",\n",
    "    \"eventsets/QCD_PU200_D49.root\"]\n",
    "\n",
    "# Create list of uproot event sets for easy data access\n",
    "event_sets = []\n",
    "for input_file in input_files:\n",
    "    event_sets.append(next(iter(uproot_open(input_file).values()))[\"eventTree\"])\n",
    "    \n",
    "# What track properties do we want available to play with?\n",
    "# We can select which ones we want a model to train on\n",
    "# Build an ntuple dict whose only track type is \"trk\"\n",
    "properties_by_track_type = {\"trk\": [\"pt\", \"eta\", \"z0\", \"nstub\", \"genuine\", \"matchtp_pdgid\",\n",
    "                                    \"chi2\", \"bendchi2\", \"chi2rphi\", \"chi2rz\"]}\n",
    "\n",
    "# Create ntuple dict from event sets\n",
    "ntuple_dict = ndops.uproot_ntuples_to_ntuple_dict(event_sets, properties_by_track_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Cuts\n",
    "\n",
    "Ensure there are as many fake tracks as there are real tracks, perform desired cuts on dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Would we like to consider only a portion of this dataset? (Typically done for speed reasons.)\n",
    "reduce_ntuple_dict = True\n",
    "reduction_size = 10000  # number of tracks to reduce to\n",
    "if reduce_ntuple_dict:\n",
    "    ntuple_dict = ndops.reduce_ntuple_dict(ntuple_dict, reduction_size, shuffle_tracks=True)\n",
    "\n",
    "# Reduce genuine track size to be equal to fake track size, concatenate and shuffle\n",
    "# (nd means ntuple_dict)\n",
    "all_nd_gens = ndops.cut_ntuple_dict(ntuple_dict, {\"trk\": {\"genuine\": sel(1)}})\n",
    "nd_fakes = ndops.cut_ntuple_dict(ntuple_dict, {\"trk\": {\"genuine\": sel(0)}})\n",
    "nd_gens = ndops.reduce_ntuple_dict(all_nd_gens,\n",
    "                                   track_limit=ndops.track_prop_dict_length(nd_fakes[\"trk\"]),\n",
    "                                   shuffle_tracks=True,\n",
    "                                   seed=42)\n",
    "nd_both = ndops.shuffle_ntuple_dict(ndops.add_ntuple_dicts([nd_gens, nd_fakes]), seed=42)\n",
    "\n",
    "# Are there any other cuts that should be applied to this dataset?\n",
    "additional_cuts = {\"trk\": {\"pt\": sel(2, 100)}}\n",
    "ntuple_dict = ndops.cut_ntuple_dict(nd_both, additional_cuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process into Datasets\n",
    "\n",
    "Process the ntuple dict above into `TrackPropertiesDataset`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_properties = [\"chi2\", \"bendchi2\", \"nstub\"]  # what properties do we want to train our models on?\n",
    "label_property = \"genuine\"                       # what property are we trying to predict?\n",
    "split_list = [.7, .2, .1]                        # how many datasets should we create,\n",
    "                                                 #   and with what relative sizes?\n",
    "train_ds, eval_ds, test_ds = ndmldata.TrackPropertiesDataset(ntuple_dict[\"trk\"],\n",
    "                                                             label_property,\n",
    "                                                             data_properties).split(split_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Models\n",
    "\n",
    "Build a neural network and a gradient boosted decision tree, train them on data. Also define a set of predictive cuts to compare our models against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "70/70 [==============================] - 1s 18ms/sample - loss: 1.8098 - accuracy: 0.5714 - val_loss: 0.7665 - val_accuracy: 0.7000\n",
      "Epoch 2/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.4870 - accuracy: 0.7714 - val_loss: 0.4381 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.4014 - accuracy: 0.8286 - val_loss: 0.3421 - val_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.3856 - accuracy: 0.8143 - val_loss: 0.3082 - val_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.3893 - accuracy: 0.8714 - val_loss: 0.2939 - val_accuracy: 0.8500\n",
      "Epoch 6/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.3834 - accuracy: 0.8857 - val_loss: 0.2910 - val_accuracy: 0.8500\n",
      "Epoch 7/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.3780 - accuracy: 0.8714 - val_loss: 0.2913 - val_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "70/70 [==============================] - 0s 2ms/sample - loss: 0.3729 - accuracy: 0.8571 - val_loss: 0.2950 - val_accuracy: 0.8500\n",
      "Epoch 9/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.3604 - accuracy: 0.8429 - val_loss: 0.2945 - val_accuracy: 0.8500\n",
      "Epoch 10/10\n",
      "70/70 [==============================] - 0s 1ms/sample - loss: 0.3555 - accuracy: 0.8429 - val_loss: 0.2916 - val_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "NN = ndmlmodels.make_neuralnet(train_ds, eval_dataset=eval_ds, hidden_layers=[15, 8], epochs=10)\n",
    "GBDT = ndmlmodels.make_gbdt(train_ds)\n",
    "cuts = {\"chi2rphi\": sel(0, 23), \"chi2rz\": sel(0, 7), \"chi2\": sel(0, 21)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions\n",
    "\n",
    "Use the test dataset `test_ds` that hasn't been used for training to make predicted labels. These will be probablistic in the case of the models `NN` and `GBDT` and exact in the case of `cuts`. Store these predictions in `test_ds` for easy future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds.add_prediction(\"NN\", ndmlpred.predict_labels(NN, test_ds.get_data()))\n",
    "test_ds.add_prediction(\"GBDT\", ndmlpred.predict_labels(GBDT, test_ds.get_data()))\n",
    "test_ds.add_prediction(\"cuts\", ndmlpred.predict_labels_cuts(cuts, test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
